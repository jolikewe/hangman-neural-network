{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Clean list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words removed: 42\n",
      "Remaining words: 227,259\n"
     ]
    }
   ],
   "source": [
    "def clean_word_list(words, min_len=2, max_len=30):\n",
    "    import re\n",
    "\n",
    "    cleaned = set()\n",
    "    for w in words:\n",
    "        w = w.strip().lower()\n",
    "        # rule 1: alphabetic only\n",
    "        if not re.fullmatch(r'[a-z]+', w):\n",
    "            continue\n",
    "        # rule 2: length constraints\n",
    "        if not (min_len <= len(w) <= max_len):\n",
    "            continue\n",
    "        # rule 3: skip words of all identical letters\n",
    "        if len(set(w)) == 1:\n",
    "            continue\n",
    "        cleaned.add(w)\n",
    "\n",
    "    print(f\"Number of words removed: {len(words) - len(cleaned):,}\")\n",
    "    print(f\"Remaining words: {len(cleaned):,}\")\n",
    "    return sorted(cleaned)\n",
    "\n",
    "\n",
    "\n",
    "file_path = \"words_train.txt\"\n",
    "max_word_len = 30\n",
    "\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    words = file.read().split('\\n')\n",
    "\n",
    "clean_words = clean_word_list(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Prepare training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import string\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "## List of words\n",
    "words = clean_words\n",
    "\n",
    "\n",
    "## Alphabets dictionary\n",
    "all_chars = list(string.ascii_lowercase)\n",
    "all_chars_len = len(all_chars)\n",
    "char_to_idx = {c: i for i, c in enumerate(all_chars)}\n",
    "idx_to_char = {i: c for i, c in enumerate(all_chars)}\n",
    "\n",
    "\n",
    "## Encoding and decoding functions\n",
    "def encode_word(word, all_chars_len=26):\n",
    "    encoded_word = torch.zeros((len(word), all_chars_len))\n",
    "\n",
    "    for i, char in enumerate(word):\n",
    "        if char == '_':\n",
    "            continue  # Skip if masked character\n",
    "        encoded_word[i, char_to_idx[char]] = 1\n",
    "    return encoded_word\n",
    "\n",
    "\n",
    "def decode_actual_word(encoded_word):\n",
    "    row_sums = encoded_word.sum(dim=1)\n",
    "    last_nonzero_idx = (row_sums != 0).nonzero(as_tuple=True)[0].max().item() + 1\n",
    "    # remove padding\n",
    "    trimmed = encoded_word[:last_nonzero_idx]\n",
    "    char_indices = trimmed.argmax(dim=1)\n",
    "\n",
    "    return ''.join(idx_to_char[i.item()] for i in char_indices)\n",
    "\n",
    "\n",
    "def decode_masked_word(masked_word, word_len):\n",
    "    trimmed = masked_word[:word_len]\n",
    "    row_sums = trimmed.sum(dim=1)\n",
    "\n",
    "    decoded_chars = []\n",
    "    for i in range(trimmed.size(0)):\n",
    "        if row_sums[i] == 0:\n",
    "            decoded_chars.append('_')\n",
    "        else:\n",
    "            idx = trimmed[i].argmax().item()\n",
    "            decoded_chars.append(idx_to_char[idx])\n",
    "    return ''.join(decoded_chars)\n",
    "\n",
    "\n",
    "## Generate training data from words\n",
    "def convert_word_to_training_data(word, all_chars_len=26):\n",
    "    encoded_word = encode_word(word, all_chars_len=all_chars_len)\n",
    "\n",
    "    # create random masking, consistent per unique character\n",
    "    unique_chars = sorted(set(word))\n",
    "    while True:\n",
    "        char_mask_map = {c: np.random.randint(0, 2) for c in unique_chars}  # 0=shown, 1=masked\n",
    "        if len(set(char_mask_map.values())) != 1:\n",
    "            break\n",
    "    mask = np.array([char_mask_map[c] for c in word])\n",
    "\n",
    "    # Apply masking directly to word length only\n",
    "    x_input = encoded_word.clone()\n",
    "    mask_tensor = torch.tensor(mask, dtype=torch.float32)\n",
    "    mask_bool = mask_tensor.bool()\n",
    "    x_input[mask_bool] = 0.0\n",
    "\n",
    "    y_target = encoded_word  # same length as x_input\n",
    "\n",
    "    return x_input, y_target, mask_tensor\n",
    "\n",
    "\n",
    "## Multithreading to generate training data\n",
    "def process_all_words(words, all_chars_len=26, cache_file=\"processed_data.pkl\", force_process=False, multiplier=2):\n",
    "    from multiprocessing.dummy import Pool\n",
    "    import pickle\n",
    "    from tqdm import tqdm\n",
    "\n",
    "    if not force_process:\n",
    "        try:\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                print(f\"Loading cached processed data from {cache_file}...\")\n",
    "                return pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"No cached data found — preprocessing...\")\n",
    "\n",
    "    # repeat each word `multiplier` times for additional training\n",
    "    words *= multiplier\n",
    "\n",
    "    # Worker to apply the conversion\n",
    "    def worker(w):\n",
    "        return convert_word_to_training_data(w, all_chars_len)\n",
    "\n",
    "    # Parallel processing\n",
    "    with Pool() as pool:\n",
    "        processed_data = list(\n",
    "            tqdm(pool.imap(worker, words), total=len(words), desc=\"Processing words\")\n",
    "        )\n",
    "\n",
    "    # Remove duplicates (tuples can be used in a set directly)\n",
    "    processed_data = list(set(processed_data))\n",
    "\n",
    "    # Cache results\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "        print(f\"Saved preprocessed data to {cache_file}\")\n",
    "\n",
    "    return processed_data\n",
    "\n",
    "\n",
    "class HangmanDataset(Dataset):\n",
    "    def __init__(self, processed_data):\n",
    "        self.data = processed_data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing words: 100%|██████████| 681777/681777 [01:04<00:00, 10601.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved preprocessed data to processed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "processed_data = process_all_words(\n",
    "    words, \n",
    "    multiplier=3, \n",
    "    cache_file=\"processed_data.pkl\", \n",
    "    force_process=False, \n",
    ")\n",
    "\n",
    "dataset = HangmanDataset(processed_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Inspect training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actual: ricey \n",
      "Masked: ri_ey \n",
      "\n",
      "Actual: resubmitting \n",
      "Masked: _esub__tt_n_ \n",
      "\n",
      "Actual: ruel \n",
      "Masked: ru_l \n",
      "\n",
      "Actual: heterosomati \n",
      "Masked: _e_er___m___ \n",
      "\n",
      "Actual: orthotomous \n",
      "Masked: o_t_oto_ou_ \n",
      "\n",
      "Actual: electrobiological \n",
      "Masked: e_ect_o__o_og_c__ \n",
      "\n",
      "Actual: dispiteous \n",
      "Masked: d_sp__eo_s \n",
      "\n",
      "Actual: tardity \n",
      "Masked: tar__t_ \n",
      "\n",
      "Actual: spiritualminded \n",
      "Masked: ______u________ \n",
      "\n",
      "Actual: barrat \n",
      "Masked: _arra_ \n",
      "\n",
      "Actual: oleums \n",
      "Masked: oleum_ \n",
      "\n",
      "Actual: ereuthalion \n",
      "Masked: _r__thali__ \n",
      "\n",
      "Actual: geobiont \n",
      "Masked: _eobiont \n",
      "\n",
      "Actual: hirundo \n",
      "Masked: _i_u_d_ \n",
      "\n",
      "Actual: drawoff \n",
      "Masked: _r__o__ \n",
      "\n",
      "Actual: tallowberries \n",
      "Masked: ta__o_b___i__ \n",
      "\n",
      "Actual: soleless \n",
      "Masked: __l_l___ \n",
      "\n",
      "Actual: isouric \n",
      "Masked: i_ou_ic \n",
      "\n",
      "Actual: kunmiut \n",
      "Masked: kun_iut \n",
      "\n",
      "Actual: wincopipe \n",
      "Masked: _in___i_e \n",
      "\n",
      "Actual: prooestrous \n",
      "Masked: pr__es_r__s \n",
      "\n",
      "Actual: blazing \n",
      "Masked: ___zi__ \n",
      "\n",
      "Actual: lunations \n",
      "Masked: lu__t____ \n",
      "\n",
      "Actual: superappreciation \n",
      "Masked: su___a____cia_io_ \n",
      "\n",
      "Actual: culicine \n",
      "Masked: c_l_c_n_ \n",
      "\n",
      "Actual: summariser \n",
      "Masked: _umm_r___r \n",
      "\n",
      "Actual: panarabic \n",
      "Masked: _a_ara___ \n",
      "\n",
      "Actual: duros \n",
      "Masked: d_ro_ \n",
      "\n",
      "Actual: phytologically \n",
      "Masked: _h___l__ic_ll_ \n",
      "\n",
      "Actual: prefectly \n",
      "Masked: pre_e__ly \n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Preview\n",
    "num_previews = 30\n",
    "\n",
    "for i in range(num_previews):\n",
    "    actual_word = decode_actual_word(processed_data[i][1])\n",
    "    masked_word = decode_masked_word(processed_data[i][0], len(actual_word))\n",
    "    print(f'Actual: {actual_word} \\nMasked: {masked_word} \\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "heterosomati\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0., 0.]])\n",
      "tensor([1., 0., 1., 0., 0., 1., 1., 1., 0., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "## Validate quality of encoded training data\n",
    "\n",
    "idx = 3\n",
    "\n",
    "print(decode_actual_word(dataset[idx][1]))\n",
    "\n",
    "# x_input\n",
    "print(dataset[idx][0])\n",
    "\n",
    "# y_input\n",
    "print(dataset[idx][1])\n",
    "\n",
    "# mask\n",
    "print(dataset[idx][2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Split data into training and validation sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import random_split, DataLoader\n",
    "\n",
    "\n",
    "def collate_fn_dynamic_padding(batch):\n",
    "    xs, ys, masks = zip(*batch)\n",
    "    lengths = [x.shape[0] for x in xs]\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    xs = pad_sequence(xs, batch_first=True) # [seq_len, feature_dim]\n",
    "    ys = pad_sequence(ys, batch_first=True)\n",
    "    masks = pad_sequence(masks, batch_first=True)\n",
    "\n",
    "    lengths, perm_idx = lengths.sort(0, descending=True)    # sort by descending length for pack_padded_sequence\n",
    "    xs, ys, masks = xs[perm_idx], ys[perm_idx], masks[perm_idx]\n",
    "    return xs, ys, masks, lengths\n",
    "\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, collate_fn=collate_fn_dynamic_padding, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, collate_fn=collate_fn_dynamic_padding, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 [Train]: 100%|██████████| 2397/2397 [04:51<00:00,  8.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 0.1241, Val Loss = 0.1129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10 [Train]: 100%|██████████| 2397/2397 [05:22<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 0.1113, Val Loss = 0.1084\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10 [Train]: 100%|██████████| 2397/2397 [04:48<00:00,  8.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 0.1079, Val Loss = 0.1059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10 [Train]: 100%|██████████| 2397/2397 [04:44<00:00,  8.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 0.1060, Val Loss = 0.1047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10 [Train]: 100%|██████████| 2397/2397 [04:44<00:00,  8.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 0.1047, Val Loss = 0.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10 [Train]: 100%|██████████| 2397/2397 [04:49<00:00,  8.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 0.1038, Val Loss = 0.1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10 [Train]: 100%|██████████| 2397/2397 [04:48<00:00,  8.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 0.1031, Val Loss = 0.1025\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10 [Train]: 100%|██████████| 2397/2397 [04:46<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 0.1024, Val Loss = 0.1021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10 [Train]: 100%|██████████| 2397/2397 [04:46<00:00,  8.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 0.1019, Val Loss = 0.1016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10 [Train]: 100%|██████████| 2397/2397 [04:38<00:00,  8.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss = 0.1015, Val Loss = 0.1013\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm import tqdm\n",
    "\n",
    "from nnModels import CustomNN\n",
    "\n",
    "\n",
    "## Define model, optimizer, loss, learning rate\n",
    "model_rnn = CustomNN(chars_len=26, embed_dim=16, hidden_dim=128, num_layers=2, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model_rnn.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# reduces LR by factor of 0.5 if val loss stagnant for 2 epochs\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "\n",
    "## Train\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model_rnn.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for x, y, mask, lengths in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        logits = model_rnn(x, lengths)\n",
    "        loss = criterion(logits[mask == 1], y[mask == 1])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # compute validation loss\n",
    "    model_rnn.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, mask, lengths in val_loader:\n",
    "            logits = model_rnn(x, lengths)\n",
    "            val_loss = criterion(logits[mask == 1], y[mask == 1])\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "## save model\n",
    "if False:\n",
    "    torch.save(model_rnn.state_dict(), \"model_rnn.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load model\n",
    "\n",
    "# import torch\n",
    "# from nnModels import CustomNN\n",
    "\n",
    "# model_rnn = CustomNN(chars_len=26, embed_dim=16, hidden_dim=128, num_layers=2, dropout=0.2)\n",
    "# model_rnn.load_state_dict(torch.load(\"model_rnn.pth\", map_location=torch.device(\"cpu\")))\n",
    "# model_rnn.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.59 MB\n"
     ]
    }
   ],
   "source": [
    "## Choose inference methodology\n",
    "\n",
    "from Hangman import HangmanRNN\n",
    "\n",
    "\n",
    "game_test = HangmanRNN(timeout=2000, model=model_rnn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next guess: 'l' at position 3\n",
      "Probability matrix (masked positions only):\n",
      "Position 3: {'f': 0.0001, 'g': 0.0011, 'i': 0.1913, 'j': 0.0003, 'k': 0.0002, 'l': 0.6618, 'm': 0.0004, 'n': 0.0014, 'o': 0.0239, 'q': 0.0, 'r': 0.0503, 's': 0.0298, 't': 0.0094, 'u': 0.0243, 'v': 0.0004, 'w': 0.0001, 'x': 0.0, 'y': 0.0051, 'z': 0.0001}\n",
      "['l', 'i', 'r', 's', 'u', 'o', 't', 'y', 'n', 'g', 'm', 'v', 'j', 'k', 'f', 'w', 'z', 'q', 'x']\n"
     ]
    }
   ],
   "source": [
    "## Example 1\n",
    "masked_word = \"a p p _ e\"\n",
    "game_test.guessed_letters = ['a', 'p', 'e', 'b', 'c', 'd', 'h']\n",
    "\n",
    "char, pos, probs = game_test.guess(masked_word, return_probs=True).values()\n",
    "\n",
    "if len(set(masked_word)) == 1:\n",
    "    print(f\"Next guess: '{char}'\")\n",
    "else:\n",
    "    print(f\"Next guess: '{char}' at position {pos[0]}\")\n",
    "    print(\"Probability matrix (masked positions only):\")\n",
    "    masked_positions = [idx for idx, char in enumerate(masked_word.replace(\" \", \"\")) if char == '_']\n",
    "    for i in masked_positions:\n",
    "        probs_dict = {idx_to_char[j]: np.round(float(probs[i, j]), 4) for j in range(all_chars_len) if probs[i,j]>0}\n",
    "        print(f\"Position {i}: {probs_dict}\")\n",
    "        print(sorted(probs_dict, key=probs_dict.get, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next guess: 'e'\n"
     ]
    }
   ],
   "source": [
    "## Example 2\n",
    "masked_word = '_ _ _ _'\n",
    "game_test.guessed_letters = []\n",
    "\n",
    "char, pos, probs = game_test.guess(masked_word, return_probs=True).values()\n",
    "\n",
    "if len(set(masked_word.replace(\" \", \"\"))) == 1:\n",
    "    print(f\"Next guess: '{char}'\")\n",
    "else:\n",
    "    print(f\"Next guess: '{char}' at position {pos[0]}\")\n",
    "    print(\"Probability matrix (masked positions only):\")\n",
    "    masked_positions = [idx for idx, char in enumerate(masked_word.replace(\" \", \"\")) if char == '_']\n",
    "    for i in masked_positions:\n",
    "        probs_dict = {idx_to_char[j]: np.round(float(probs[i, j]), 4) for j in range(all_chars_len) if probs[i,j]>0}\n",
    "        print(f\"Position {i}: {probs_dict}\")\n",
    "        print(sorted(probs_dict, key=probs_dict.get, reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
