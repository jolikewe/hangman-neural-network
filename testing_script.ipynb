{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from nnModels import CustomNN\n",
    "\n",
    "model = CustomNN(chars_len=26, embed_dim=16, hidden_dim=128, num_layers=2, dropout=0.2)\n",
    "model.load_state_dict(torch.load(\"model_rnn.pth\", map_location=torch.device(\"cpu\")))\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Load inference methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Hangman import HangmanBasic, HangmanRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Inference\n",
    "def guess(model, masked_word, guessed_letters):\n",
    "    stripped = masked_word.replace(\" \", \"\")\n",
    "    if all(c == '_' for c in stripped):\n",
    "        # First guess: prioritize vowels not yet guessed\n",
    "        vowels = ['e', 'a', 'o', 'i', 'u']\n",
    "        for v in vowels:\n",
    "            if v not in guessed_letters:\n",
    "                return v, None, None\n",
    "        # If all vowels are wrong, fall back to consonants (ETAOIN SHRDLU consonants)\n",
    "        sorted_consonants = ['t', 'n', 's', 'h', 'r', 'd', 'l', 'b', 'c', 'f', 'g', 'j', 'k', 'm', 'p', 'q', 'v', 'w', 'x', 'y', 'z']\n",
    "        for c in sorted_consonants:\n",
    "            if c not in guessed_letters:\n",
    "                return c, None, None\n",
    "\n",
    "    word_len = len(stripped)\n",
    "    x_input = encode_word(stripped)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        lengths = torch.tensor([word_len])  # batch of 1\n",
    "        logits = model(x_input.unsqueeze(0), lengths)\n",
    "        probs = torch.softmax(logits[0], dim=-1)    # max_word_len x all_chars_len\n",
    "        \n",
    "        # 1. Zero out positions already known (non-zero in input)\n",
    "        known_positions_mask = x_input.sum(dim=1) > 0  # [T]\n",
    "        probs[known_positions_mask] = 0.0\n",
    "\n",
    "        # 2. Zero out previously guessed letters\n",
    "        if guessed_letters:\n",
    "            guessed_char_idx = torch.tensor([char_to_idx.get(x) for x in guessed_letters])\n",
    "            unknown_positions_mask = torch.tensor([i for i in range(word_len) if not known_positions_mask[i]])\n",
    "            probs[unknown_positions_mask[:, None], guessed_char_idx] = 0\n",
    "\n",
    "        # 3. normalize probabilities within each position\n",
    "        row_sums = probs.sum(dim=1, keepdim=True) + 1e-8  # avoid division by zero\n",
    "        probs_normalized = probs / row_sums\n",
    "\n",
    "        # 4. pick max probability among all positions and characters\n",
    "        guessed_pos, char = torch.where(probs_normalized == probs_normalized.max())\n",
    "        guessed_char = idx_to_char[char[0].item()]\n",
    "\n",
    "    return guessed_char, guessed_pos, probs_normalized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next guess: 'l' at position tensor([3])\n",
      "Probability matrix (masked positions only):\n",
      "Position 3:  {'f': 0.00012836948735639453, 'g': 0.001108763855881989, 'i': 0.19128884375095367, 'j': 0.0002790417929645628, 'k': 0.00015477229317184538, 'l': 0.6618263721466064, 'm': 0.00042593933176249266, 'n': 0.001427668146789074, 'o': 0.023885030299425125, 'q': 2.3676307137066033e-06, 'r': 0.05030466243624687, 's': 0.029816830530762672, 't': 0.00936949159950018, 'u': 0.024286340922117233, 'v': 0.0003797242825385183, 'w': 0.00012386047455947846, 'x': 1.4550921150657814e-05, 'y': 0.005062872543931007, 'z': 0.00011447598080849275}\n",
      "['l', 'i', 'r', 's', 'u', 'o', 't', 'y', 'n', 'g', 'm', 'v', 'j', 'k', 'f', 'w', 'z', 'x', 'q']\n"
     ]
    }
   ],
   "source": [
    "## Example 1\n",
    "masked_word = 'app_e'\n",
    "hangman_input = ' '.join(masked_word)\n",
    "guessed_letters = ['a', 'p', 'e', 'b', 'c', 'd', 'h']\n",
    "\n",
    "\n",
    "char, pos, probs = guess(model, hangman_input, guessed_letters)\n",
    "\n",
    "if len(set(masked_word)) == 1:\n",
    "    print(f\"Next guess: '{char}'\")\n",
    "else:\n",
    "    print(f\"Next guess: '{char}' at position {pos}\")\n",
    "    print(\"Probability matrix (masked positions only):\")\n",
    "    masked_positions = [idx for idx, char in enumerate(masked_word) if char == '_']\n",
    "    for i in masked_positions:\n",
    "        probs_dict = {idx_to_char[j]: float(probs[i, j]) for j in range(all_chars_len) if probs[i,j]>0}\n",
    "        print(f\"Position {i}: \", probs_dict)\n",
    "        print(sorted(probs_dict, key=probs_dict.get, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next guess: 'e'\n"
     ]
    }
   ],
   "source": [
    "## Example 2\n",
    "masked_word = '____'\n",
    "hangman_input = ' '.join(masked_word)\n",
    "guessed_letters = []\n",
    "\n",
    "\n",
    "char, pos, probs = guess(model, hangman_input, guessed_letters)\n",
    "\n",
    "if len(set(masked_word)) == 1:\n",
    "    print(f\"Next guess: '{char}'\")\n",
    "else:\n",
    "    print(f\"Next guess: '{char}' at position {pos}\")\n",
    "    print(\"Probability matrix (masked positions only):\")\n",
    "    masked_positions = [idx for idx, char in enumerate(masked_word) if char == '_']\n",
    "    for i in masked_positions:\n",
    "        probs_dict = {idx_to_char[j]: float(probs[i, j]) for j in range(all_chars_len) if probs[i,j]>0}\n",
    "        print(f\"Position {i}: \", probs_dict)\n",
    "        print(sorted(probs_dict, key=probs_dict.get, reverse=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing recorded games:\n",
    "Please finalize your code prior to running the cell below. Once this code executes once successfully your submission will be finalized. Our system will not allow you to rerun any additional games.\n",
    "\n",
    "Please note that it is expected that after you successfully run this block of code that subsequent runs will result in the error message \"Your account has been deactivated\".\n",
    "\n",
    "Once you've run this section of the code your submission is complete. Please send us your source code via email."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Playing  0  th game\n",
      "Playing  1  th game\n",
      "Playing  2  th game\n",
      "Playing  3  th game\n",
      "Playing  4  th game\n",
      "Playing  5  th game\n",
      "Playing  6  th game\n",
      "Playing  7  th game\n",
      "Playing  8  th game\n",
      "Playing  9  th game\n",
      "Playing  10  th game\n",
      "Playing  11  th game\n",
      "Playing  12  th game\n",
      "Playing  13  th game\n",
      "Playing  14  th game\n",
      "Playing  15  th game\n",
      "Playing  16  th game\n",
      "Playing  17  th game\n",
      "Playing  18  th game\n",
      "Playing  19  th game\n",
      "Playing  20  th game\n",
      "Playing  21  th game\n",
      "Playing  22  th game\n",
      "Playing  23  th game\n",
      "Playing  24  th game\n",
      "Playing  25  th game\n",
      "Playing  26  th game\n"
     ]
    },
    {
     "ename": "HangmanAPIError",
     "evalue": "{'error': 'You have reached 1000 of games', 'status': 'denied'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHangmanAPIError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPlaying \u001b[39m\u001b[38;5;124m'\u001b[39m, i, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m th game\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[43mapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart_game\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpractice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\u001b[39;00m\n\u001b[1;32m      7\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.5\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 169\u001b[0m, in \u001b[0;36mHangmanAPI.start_game\u001b[0;34m(self, practice, verbose)\u001b[0m\n\u001b[1;32m    166\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mguessed_letters \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_dictionary \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfull_dictionary\n\u001b[0;32m--> 169\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/new_game\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpractice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mpractice\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapproved\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    171\u001b[0m     game_id \u001b[38;5;241m=\u001b[39m response\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgame_id\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 271\u001b[0m, in \u001b[0;36mHangmanAPI.request\u001b[0;34m(self, path, args, post_args, method)\u001b[0m\n\u001b[1;32m    268\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMaintype was not text, or querystring\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    270\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(result, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m result\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 271\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HangmanAPIError(result)\n\u001b[1;32m    272\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mHangmanAPIError\u001b[0m: {'error': 'You have reached 1000 of games', 'status': 'denied'}"
     ]
    }
   ],
   "source": [
    "for i in range(1000):\n",
    "    print('Playing ', i, ' th game')\n",
    "    # Uncomment the following line to execute your final runs. Do not do this until you are satisfied with your submission\n",
    "    api.start_game(practice=0,verbose=False)\n",
    "    \n",
    "    # DO NOT REMOVE as otherwise the server may lock you out for too high frequency of requests\n",
    "    time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To check your game statistics\n",
    "1. Simply use \"my_status\" method.\n",
    "2. Returns your total number of games, and number of wins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "overall success rate = 0.517\n"
     ]
    }
   ],
   "source": [
    "[total_practice_runs,total_recorded_runs,total_recorded_successes,total_practice_successes] = api.my_status() # Get my game stats: (# of tries, # of wins)\n",
    "success_rate = total_recorded_successes/total_recorded_runs\n",
    "print('overall success rate = %.3f' % success_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ANNEX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model 1 - basic\n",
    "\n",
    "class HangmanRNN(nn.Module):\n",
    "    def __init__(self, chars_len=26, embed_dim=16, hidden_dim=256, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(chars_len, embed_dim)\n",
    "        self.rnn = nn.GRU(embed_dim, hidden_dim, num_layers=num_layers, \n",
    "                          batch_first=True, dropout=dropout)#, bidirectional=True)\n",
    "        self.norm = nn.LayerNorm(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim, chars_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)   # num_batches x max_word_len x embed_dim\n",
    "        out, _ = self.rnn(x)    # num_batches x max_word_len x hidden_dim\n",
    "        out = self.norm(out)\n",
    "        logits = self.fc(out)   # num_batches x max_word_len x all_chars_len\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "## Model 2 - added bidirectional training\n",
    "\n",
    "class HangmanRNN(nn.Module):\n",
    "    def __init__(self, chars_len=26, embed_dim=16, hidden_dim=256, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(chars_len, embed_dim)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)    # bidirectional doubles hidden size\n",
    "        self.fc = nn.Linear(hidden_dim * 2, chars_len)\n",
    "        self.embed_dropout = nn.Dropout(dropout)    # Optional dropout after embedding\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.embed_dropout(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.norm(out)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "## Model 3 - added training-validation split\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "\n",
    "## Model 4 - added learning rate scheduler\n",
    "\n",
    "model4 = HangmanRNN(chars_len=26, embed_dim=16, hidden_dim=128, num_layers=2, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "\n",
    "\n",
    "## Model 5 -- added dynamic padding so max_seq_len depends on max word length within each batch\n",
    "\n",
    "def collate_fn_dynamic_padding(batch):\n",
    "    xs, ys, masks = zip(*batch)\n",
    "    lengths = [x.shape[0] for x in xs]\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # pad_sequence expects a list of tensors [seq_len, feature_dim]\n",
    "    xs = pad_sequence(xs, batch_first=True)\n",
    "    ys = pad_sequence(ys, batch_first=True)\n",
    "    masks = pad_sequence(masks, batch_first=True)\n",
    "\n",
    "    # sort by descending length for pack_padded_sequence\n",
    "    lengths, perm_idx = lengths.sort(0, descending=True)\n",
    "    xs, ys, masks = xs[perm_idx], ys[perm_idx], masks[perm_idx]\n",
    "    return xs, ys, masks, lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data processing with max word length\n",
    "\n",
    "def encode_word(word, max_word_len=30, all_chars_len=26):\n",
    "    word_len = len(word)\n",
    "    encoded_word = torch.zeros((max_word_len, all_chars_len))\n",
    "\n",
    "    for i, char in enumerate(word):\n",
    "        if char == '_':\n",
    "            continue  # skip if masked character\n",
    "        encoded_word[i, char_to_idx[char]] = 1\n",
    "    return encoded_word, word_len\n",
    "\n",
    "\n",
    "def convert_word_to_training_data(word, max_word_len=30, all_chars_len=26):\n",
    "    encoded_word, word_len = encode_word(word, max_word_len, all_chars_len)\n",
    "\n",
    "    # create random masking, but ensures consistent masking per unique character\n",
    "    unique_chars = sorted(set(word))\n",
    "    while True:\n",
    "        char_mask_map = {c: np.random.randint(0, 2) for c in unique_chars}  # 0=shown, 1=masked\n",
    "        if len(set(char_mask_map.values())) != 1:   # ensure not all masked/unmasked\n",
    "            break\n",
    "    mask = np.array([char_mask_map[c] for c in word])\n",
    "\n",
    "    # apply masking\n",
    "    mask_tensor = torch.zeros(max_word_len, dtype=torch.float32)\n",
    "    mask_tensor[:word_len] = torch.tensor(mask, dtype=torch.float32)\n",
    "\n",
    "    # zero-out masked positions\n",
    "    mask_bool = mask_tensor.bool()\n",
    "    x_input = encoded_word.clone()\n",
    "    mask_full = torch.zeros(max_word_len, dtype=torch.bool) # pad mask_bool to full length first (30)\n",
    "    mask_full[:word_len] = mask_bool[:word_len]\n",
    "    x_input[mask_full] = 0.0\n",
    "\n",
    "    y_target = encoded_word  # same\n",
    "\n",
    "    return x_input, y_target, mask_tensor\n",
    "\n",
    "\n",
    "def process_all_words(words, max_word_len=30, all_chars_len=26, cache_file=\"processed.pkl\", force_process=False):\n",
    "    from multiprocessing.dummy import Pool\n",
    "    import pickle\n",
    "    \n",
    "    if not force_process:\n",
    "        try:\n",
    "            with open(cache_file, \"rb\") as f:\n",
    "                print(f\"Loading cached preprocessed data from {cache_file}...\")\n",
    "                return pickle.load(f)\n",
    "        except FileNotFoundError:\n",
    "            print(\"No cached data found — preprocessing...\")\n",
    " \n",
    "    def worker(w):\n",
    "        return convert_word_to_training_data(w, max_word_len, all_chars_len)\n",
    "\n",
    "    with Pool() as pool:\n",
    "        processed_data = list(\n",
    "            tqdm(pool.imap(worker, words), total=len(words), desc=\"Preprocessing words\")\n",
    "        )\n",
    "\n",
    "    # cache results for future runs\n",
    "    with open(cache_file, \"wb\") as f:\n",
    "        pickle.dump(processed_data, f)\n",
    "        print(f\"Saved preprocessed data to {cache_file}\")\n",
    "\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model\n",
    "\n",
    "class HangmanRNN(nn.Module):\n",
    "    def __init__(self, chars_len=26, embed_dim=16, hidden_dim=128, num_layers=2, dropout=0.2):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Linear(chars_len, embed_dim)\n",
    "        self.rnn = nn.GRU(\n",
    "            embed_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.norm = nn.LayerNorm(hidden_dim * 2)    # bidirectional doubles hidden size\n",
    "        self.fc = nn.Linear(hidden_dim * 2, chars_len)\n",
    "        self.embed_dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = self.embed_dropout(x)\n",
    "        out, _ = self.rnn(x)\n",
    "        out = self.norm(out)\n",
    "        logits = self.fc(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "\n",
    "## Split into training and val\n",
    "dataset_size = len(dataset)\n",
    "val_size = int(0.1 * dataset_size)\n",
    "train_size = dataset_size - val_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "\n",
    "## Define model, optimizer, loss, learning rate\n",
    "model4 = HangmanRNN(chars_len=26, embed_dim=16, hidden_dim=128, num_layers=2, dropout=0.2)\n",
    "optimizer = torch.optim.Adam(model4.parameters(), lr=1e-3)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "# reduces LR by factor of 0.5 if val loss stagnant for 2 epochs\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)\n",
    "\n",
    "\n",
    "## Train\n",
    "num_epochs = 6\n",
    "for epoch in range(num_epochs):\n",
    "    model4.train()\n",
    "    total_train_loss = 0\n",
    "\n",
    "    for x, y, mask in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\"):\n",
    "        logits = model4(x)\n",
    "        loss = criterion(logits[mask == 1], y[mask == 1])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "    # compute validation loss\n",
    "    model4.eval()\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for x, y, mask in val_loader:\n",
    "            logits = model4(x)\n",
    "            val_loss = criterion(logits[mask == 1], y[mask == 1])\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_loader)\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n",
    "\n",
    "\n",
    "## Inference\n",
    "def guess(model, masked_word, guessed_letters):\n",
    "\n",
    "    stripped = masked_word.replace(\" \", \"\")\n",
    "    if all(c == '_' for c in stripped):\n",
    "        # First guess: prioritize vowels not yet guessed\n",
    "        vowels = ['e', 'a', 'o', 'i', 'u']\n",
    "        for v in vowels:\n",
    "            if v not in guessed_letters:\n",
    "                return v, None, None\n",
    "        # If all vowels are wrong, fall back to consonants (ETAOIN SHRDLU consonants)\n",
    "        sorted_consonants = ['t', 'n', 's', 'h', 'r', 'd', 'l', 'b', 'c', 'f', 'g', 'j', 'k', 'm', 'p', 'q', 'v', 'w', 'x', 'y', 'z']\n",
    "        for c in sorted_consonants:\n",
    "            if c not in guessed_letters:\n",
    "                return c, None, None\n",
    "\n",
    "    x_input, word_len = encode_word(stripped)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        logits = model(x_input.unsqueeze(0))        # 1 x max_word_len x all_chars_len\n",
    "        probs = torch.softmax(logits[0], dim=-1)    # max_word_len x all_chars_len\n",
    "        \n",
    "        # 1. Zero out positions already known (non-zero in input)\n",
    "        x_input = x_input[:word_len]\n",
    "        probs = probs[:word_len]\n",
    "\n",
    "        known_positions_mask = x_input.sum(dim=1) > 0  # [T]\n",
    "        probs[known_positions_mask] = 0.0\n",
    "\n",
    "        # 2. Zero out previously guessed letters\n",
    "        if guessed_letters:\n",
    "            guessed_char_idx = torch.tensor([char_to_idx.get(x) for x in guessed_letters])\n",
    "            unknown_positions_mask = torch.tensor([i for i in range(word_len) if not known_positions_mask[i]])\n",
    "            probs[unknown_positions_mask[:, None], guessed_char_idx] = 0\n",
    "\n",
    "        # 3. Remove padding predictions\n",
    "        probs = probs[:word_len]\n",
    "\n",
    "        # 4. normalize probabilities within each position\n",
    "        row_sums = probs.sum(dim=1, keepdim=True) + 1e-8  # avoid division by zero\n",
    "        probs_normalized = probs / row_sums\n",
    "\n",
    "        # 5. pick max probability among all positions and characters\n",
    "        pos, char = torch.where(probs_normalized == probs_normalized.max())\n",
    "        guessed_char = idx_to_char[char[0].item()]\n",
    "        guessed_pos = pos[0].item()\n",
    "\n",
    "    return guessed_char, guessed_pos, probs_normalized\n",
    "\n",
    "\n",
    "\n",
    "## Example\n",
    "masked_word = 'app_e'\n",
    "hangman_input = ' '.join(masked_word)\n",
    "guessed_letters = ['a', 'p', 'e', 'b', 'c', 'd', 'h']\n",
    "\n",
    "\n",
    "char, pos, probs = guess(model4, hangman_input, guessed_letters)\n",
    "\n",
    "if len(set(masked_word)) == 1:\n",
    "    print(f\"Next guess: '{char}'\")\n",
    "else:\n",
    "    print(f\"Next guess: '{char}' at position {pos}\")\n",
    "    print(\"Probability matrix (masked positions only):\")\n",
    "    masked_positions = [idx for idx, char in enumerate(masked_word) if char == '_']\n",
    "    for i in masked_positions:\n",
    "        probs_dict = {idx_to_char[j]: float(probs[i, j]) for j in range(all_chars_len) if probs[i,j]>0}\n",
    "        print(f\"Position {i}: \", probs_dict)\n",
    "        print(sorted(probs_dict, key=probs_dict.get, reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
